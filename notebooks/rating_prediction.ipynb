{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3a39627a93c63",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "books_df = pd.read_csv('../data/books.csv')\n",
    "clustered_users_df = pd.read_csv('../data/produced/clustered_users.csv')\n",
    "\n",
    "# Merge the dataframes on ISBN\n",
    "data_df = pd.merge(clustered_users_df, books_df, on='isbn', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizing the summaries\n",
    "data_df['tokenized_summary'] = data_df['summary'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Training Word2Vec model\n",
    "model = Word2Vec(sentences=data_df['tokenized_summary'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert summaries to vectors\n",
    "def summary_to_vec(tokens):\n",
    "    vec = np.mean([model.wv[word] for word in tokens if word in model.wv.key_to_index], axis=0)\n",
    "    return vec\n",
    "\n",
    "data_df['summary_vec'] = data_df['tokenized_summary'].apply(summary_to_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "44bf552401a84c41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = np.array(data_df['summary_vec'].tolist())\n",
    "y = data_df['cluster'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=100, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))  # Assuming there are 5 clusters in total\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a413f63d2a097d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "985e17ab58530cc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_df['tokenized_summary'] = unlabeled_df['summary'].apply(lambda x: word_tokenize(str(x)))\n",
    "unlabeled_df['summary_vec'] = unlabeled_df['tokenized_summary'].apply(summary_to_vec)\n",
    "\n",
    "X_unlabeled = np.array(unlabeled_df['summary_vec'].tolist())\n",
    "\n",
    "# Predict clusters\n",
    "predicted_clusters = np.argmax(model.predict(X_unlabeled), axis=-1)\n",
    "unlabeled_df['predicted_cluster'] = predicted_clusters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
